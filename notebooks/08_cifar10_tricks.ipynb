{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbTYFRhJoaBu"
      },
      "source": [
        "# Cifar10 classification with and without normalization\n",
        "\n",
        "In this notebook you will download the cifar10 dataset which contains quite small images (32x32x3) of 10 classes. The data is from the Canadian Institute For Advanced Research. You will plot examples of the images with the class label. Note that because the images are so small it is not always very easy to recognise which of the ten classes is on the image, even as a human. After loading the dataset you will train multiple models and compare the performances of the models on the testset.\n",
        "\n",
        "**Dataset:**  You work with the Cifar10 dataset. You have 60'000 32x32 pixel color images of 10 classes (\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\")\n",
        "\n",
        "**Content:**\n",
        "* load the original cifar10 data create a train val and test dataset\n",
        "* visualize samples of cifar10 dataset\n",
        "\n",
        "* train a random forest on the pixelvalues\n",
        "* train a random forest on the vgg16 features of the images\n",
        "* use transfer learning with the pretrained vgg16 network\n",
        "* train a cnn from scratch\n",
        "* train a cnn from scratch with dropout\n",
        "* train a cnn from scratch with batchnorm\n",
        "* train a cnn from scratch with data augmentation\n",
        "\n",
        "* compare the performances of the models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEIS4WvpsT5t"
      },
      "source": [
        "#### Imports\n",
        "\n",
        "In the next two cells, we load all the required libraries and functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0u353ydb9w2",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# load required libraries:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "%matplotlib inline\n",
        "plt.style.use('default')\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten , Activation, Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import optimizers\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7kfDCYsxRVJ"
      },
      "source": [
        "\n",
        "### Load and plot the data\n",
        "\n",
        "In the next cell you will load the Cifar10 dataset, 50'000 images are in the training set and 10'000 are in the test dataset. You will use 10'000 for the train and validation dataset.\n",
        "You will plot one random example of each label and will see\n",
        "that the images are really small and finally you can convert the lables into the one hot encoding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRLfhCzVviwL"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zwRlaILb9xF"
      },
      "outputs": [],
      "source": [
        "# separate train val and test dataset\n",
        "X_train=x_train[0:10000]\n",
        "Y_train=to_categorical(y_train[0:10000],10) # one-hot encoding\n",
        "\n",
        "X_val=x_train[20000:30000]\n",
        "Y_val=to_categorical(y_train[20000:30000],10)\n",
        "\n",
        "X_test=x_test\n",
        "Y_test=to_categorical(y_test,10)\n",
        "\n",
        "del x_train, y_train, x_test, y_test\n",
        "\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6jukEcMb9xL"
      },
      "outputs": [],
      "source": [
        "labels=np.array([\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"])\n",
        "#sample image of each label\n",
        "plt.figure(figsize=(15,15))\n",
        "for i in range(0,len(np.unique(np.argmax(Y_train,axis=1)))):\n",
        "    rmd=np.random.choice(np.where(np.argmax(Y_train,axis=1)==i)[0],1)\n",
        "    plt.subplot(1,10,i+1)\n",
        "    img=X_train[rmd]\n",
        "    plt.imshow(img[0,:,:,:])\n",
        "    plt.title(labels[i]+\" \"+str(np.argmax(Y_train,axis=1)[rmd][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLX7w_4Zb9xP"
      },
      "outputs": [],
      "source": [
        "# check the shape of the data\n",
        "X_train.shape, Y_train.shape, X_val.shape, Y_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EAsMG-WcKms"
      },
      "outputs": [],
      "source": [
        "# normalization\n",
        "X_train = X_train/255\n",
        "X_val = X_val/255\n",
        "X_test = X_test/255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5rxBKcRdPKR"
      },
      "source": [
        "### RF on pixelvalues\n",
        "In this section you will train a random forest on the raw pixelvalues of the images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQnu_hIpcUp1"
      },
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(n_estimators=40,random_state=22)\n",
        "clf.fit(X_train.reshape(len(X_train),32*32*3), np.argmax(Y_train,axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY9AkXRrcUp-"
      },
      "outputs": [],
      "source": [
        "pred=clf.predict(X_test.reshape(len(X_test),32*32*3))\n",
        "acc=np.average(pred==np.argmax(Y_test,axis=1))\n",
        "res1 = pd.DataFrame(\n",
        "          {'Acc' : acc}, index=['rf on pixelvalues'])\n",
        "res1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi-o6-OagtZf"
      },
      "source": [
        "### CNN from scratch\n",
        "In this section you train a cnn from scratch to learn to classify the images into the right label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlXRWiwi8FHq"
      },
      "outputs": [],
      "source": [
        "model  =  Sequential()\n",
        "\n",
        "model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\",input_shape=(32,32,3)))\n",
        "model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\"))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n",
        "model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(300))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(100))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7B65EQTg6vM"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, Y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=10, validation_data=(X_val, Y_val),verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aud3gKbNhT3b"
      },
      "outputs": [],
      "source": [
        "# plot the development of the accuracy and loss during training\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,(1))\n",
        "plt.plot(history.history['accuracy'],linestyle='-.')\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='lower right')\n",
        "plt.subplot(1,2,(2))\n",
        "plt.plot(history.history['loss'],linestyle='-.')\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYco3ZFohT3r"
      },
      "outputs": [],
      "source": [
        "acc=np.average(np.argmax(model.predict(X_test),axis=1)==np.argmax(Y_test,axis=1))\n",
        "res2 = pd.DataFrame(\n",
        "          {'Acc' : acc}, index=['cnn from scratch']\n",
        ")\n",
        "pd.concat([res1,res2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIZH1xpehpOH"
      },
      "source": [
        "### CNN from scratch with Dropout\n",
        "In this section you train a cnn from scratch to learn to classify the images into the right label. This time you will use dropout layers in the classification part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYD-TYOzhpOM"
      },
      "outputs": [],
      "source": [
        "model  =  Sequential()\n",
        "\n",
        "model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\",input_shape=(32,32,3)))\n",
        "model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\"))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n",
        "model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(500))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(300))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(100))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfv_8EYChpOR"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, Y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=20, validation_data=(X_val, Y_val),verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4utU_Wd7hpOY"
      },
      "outputs": [],
      "source": [
        "# plot the development of the accuracy and loss during training\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,(1))\n",
        "plt.plot(history.history['accuracy'],linestyle='-.')\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='lower right')\n",
        "plt.subplot(1,2,(2))\n",
        "plt.plot(history.history['loss'],linestyle='-.')\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qAipwYkhpOf"
      },
      "outputs": [],
      "source": [
        "acc = np.average(np.argmax(model.predict(X_test),axis=1)==np.argmax(Y_test,axis=1))\n",
        "res3 = pd.DataFrame(\n",
        "          {'Acc' : acc}, index=['cnn from scratch with dropout']\n",
        ")\n",
        "pd.concat([res1,res2,res3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP_oH_fcih0D"
      },
      "source": [
        "### CNN from scratch with Batchnorm\n",
        "In this section you train a cnn from scratch to learn to classify the images into the right label. This time you will use batchnorm on the input and in the convolutional part of the network. Note that we reload the original images and do not normalize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmabrMdGkTLx"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# separate train val and test dataset\n",
        "X_train_=x_train[0:10000]\n",
        "Y_train=to_categorical(y_train[0:10000],10) # one-hot encoding\n",
        "\n",
        "X_val_=x_train[20000:30000]\n",
        "Y_val=to_categorical(y_train[20000:30000],10)\n",
        "\n",
        "X_test_=x_test\n",
        "Y_test=to_categorical(y_test,10)\n",
        "\n",
        "del x_train, y_train, x_test, y_test\n",
        "\n",
        "\n",
        "print(X_train_.shape)\n",
        "print(X_val_.shape)\n",
        "print(X_test_.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZCaUy0PkTL7"
      },
      "outputs": [],
      "source": [
        "# not normalized, values between 0 and 255\n",
        "X_train_[0,0:10,0,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLQ-RZgxih0J"
      },
      "outputs": [],
      "source": [
        "model  =  Sequential()\n",
        "\n",
        "model.add(BatchNormalization(input_shape=(32,32,3)))\n",
        "model.add(Convolution2D(16,(3,3),padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(16,(3,3),padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Convolution2D(32,(3,3),padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(32,(3,3),padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(300))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(100))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f5HV12Pih0Q"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train_, Y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=10, validation_data=(X_val_, Y_val),verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar8YM_44ih0Y"
      },
      "outputs": [],
      "source": [
        "# plot the development of the accuracy and loss during training\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,(1))\n",
        "plt.plot(history.history['accuracy'],linestyle='-.')\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='lower right')\n",
        "plt.subplot(1,2,(2))\n",
        "plt.plot(history.history['loss'],linestyle='-.')\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgqm4Erdih0e"
      },
      "outputs": [],
      "source": [
        "acc = np.average(np.argmax(model.predict(X_test_),axis=1)==np.argmax(Y_test,axis=1))\n",
        "res4 = pd.DataFrame(\n",
        "          {'Acc' : acc}, index=['cnn from scratch with batchnorm']\n",
        ")\n",
        "pd.concat([res1,res2,res3,res4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y12h3tb5g4r4"
      },
      "source": [
        "#### Exercise\n",
        "Calculate the confusion matrix of the networks.  \n",
        "Play around with the dropout rate and the position of the batchnorm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBIqHwZMgI75"
      },
      "source": [
        "## Learning with few data\n",
        "In case of few data you can work with features that you extract from a pretrained cnn. Data augmentation inceases the training data and usually help to improve the performace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPx0Bs50ej4j"
      },
      "source": [
        "### Baseline model: RF on VGG features\n",
        "In this section you use a pretrained vgg16 network that was trained on the imagenet data as a image feature extractor and then train a random forest on these features. You will extract a 512 dimensional vector from each image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vow2NgC719CZ"
      },
      "outputs": [],
      "source": [
        "# load the pretrained vgg model\n",
        "base_model = tf.keras.applications.vgg16.VGG16(weights='imagenet',\n",
        "                                               include_top=False,\n",
        "                                               input_shape=(32,32,3),\n",
        "                                               pooling=\"avg\")\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J61OHaDf19RB"
      },
      "outputs": [],
      "source": [
        "# extract the vgg features of the images\n",
        "X_train_vgg_features=base_model.predict(X_train)\n",
        "X_val_vgg_features=base_model.predict(X_val)\n",
        "X_test_vgg_features=base_model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiZX32Dn19Zc"
      },
      "outputs": [],
      "source": [
        "# train a random forest on the vgg features\n",
        "clf = RandomForestClassifier(n_estimators=40,random_state=22)\n",
        "clf.fit(X_train_vgg_features, np.argmax(Y_train,axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjVW19Dx19WE"
      },
      "outputs": [],
      "source": [
        "# use the trained vgg features to predict the test features\n",
        "pred=clf.predict(X_test_vgg_features)\n",
        "acc=np.average(pred==np.argmax(Y_test,axis=1))\n",
        "res5 = pd.DataFrame(\n",
        "          {'Acc' : acc}, index=['rf on vgg features']\n",
        ")\n",
        "pd.concat([res1,res2,res3,res4,res5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBNDnWh-fVDN"
      },
      "source": [
        "### Transfer learning\n",
        "In this section you use a pretrained vgg16 network that was trained on the imagenet and keep the weights of the convolutional part fixed. You will add your own classification part (fully connected layers) on top of the imagetnet-learned convolutional part. Only the classification part will be learned, we fix all other weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNimnD7A39uU"
      },
      "outputs": [],
      "source": [
        "base_model = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(32,32,3))\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(400, activation='relu')(x)\n",
        "x = Dense(200, activation='relu')(x)\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxG3Q7Wf39zk"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLp0Htz539r8"
      },
      "outputs": [],
      "source": [
        "# freeze the weights of the convolutional part\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "for i, layer in enumerate(model.layers):\n",
        "   print(i, layer.name,layer.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F159Vo_E39o1"
      },
      "outputs": [],
      "source": [
        "history=model.fit(X_train, Y_train,\n",
        "                  batch_size=64,\n",
        "                  epochs=15,\n",
        "                  verbose=1,\n",
        "                  shuffle=True,\n",
        "                  validation_data=(X_val, Y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJssR3uU6I2S"
      },
      "outputs": [],
      "source": [
        "# plot the development of the accuracy and loss during training\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,(1))\n",
        "plt.plot(history.history['accuracy'],linestyle='-.')\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='lower right')\n",
        "plt.subplot(1,2,(2))\n",
        "plt.plot(history.history['loss'],linestyle='-.')\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rCQihzV189V"
      },
      "outputs": [],
      "source": [
        "acc = np.average(np.argmax(model.predict(X_test),axis=1)==np.argmax(Y_test,axis=1))\n",
        "res6 = pd.DataFrame(\n",
        "          {'Acc' : acc}, index=['transfer learning on vgg features']\n",
        ")\n",
        "pd.concat([res1,res2,res3,res4,res5,res6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiKVO-uOlMbg"
      },
      "source": [
        "### CNN from scratch with Data Augmentation\n",
        "In this section you train a cnn from scratch to learn to classify the images into the right label. This time you will use data augmentation, so the network will train on slightly different versions of the images in each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXFfkFM5lMbn"
      },
      "outputs": [],
      "source": [
        "model  =  Sequential()\n",
        "\n",
        "model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\",input_shape=(32,32,3)))\n",
        "model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\"))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n",
        "model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(300))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(100))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWzxK6id8FPp"
      },
      "outputs": [],
      "source": [
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    shear_range=0.15,\n",
        "    zoom_range=0.15,\n",
        "fill_mode=\"constant\",\n",
        "cval=1,horizontal_flip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQD2VP7s8FMw"
      },
      "outputs": [],
      "source": [
        "i=22\n",
        "data_aug=datagen.flow(x=X_train[i:(i+1)], y=Y_train[i:(i+1)], batch_size=1)\n",
        "plt.imshow(X_train[i])\n",
        "plt.show()\n",
        "plt.figure(figsize=(15,15))\n",
        "for i in range (0,25):\n",
        "  plt.subplot(5,5,i+1)\n",
        "  x_aug,y_aug=next(data_aug)\n",
        "  plt.imshow(x_aug[0,:,:,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgSM-3XT8FE1"
      },
      "outputs": [],
      "source": [
        "history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=64),\n",
        "                              steps_per_epoch=len(X_train)/64,\n",
        "                              epochs=40,\n",
        "                              validation_data=(X_val, Y_val),\n",
        "                              verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19jPWWNeDBgR"
      },
      "outputs": [],
      "source": [
        "# plot the development of the accuracy and loss during training\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,(1))\n",
        "plt.plot(history.history['accuracy'],linestyle='-.')\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='lower right')\n",
        "plt.subplot(1,2,(2))\n",
        "plt.plot(history.history['loss'],linestyle='-.')\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUiqoT9gDafA"
      },
      "outputs": [],
      "source": [
        "acc = np.average(np.argmax(model.predict(X_test),axis=1)==np.argmax(Y_test,axis=1))\n",
        "res7 = pd.DataFrame(\n",
        "          {'Acc' : acc}, index=['cnn from scratch with data augmentation']\n",
        ")\n",
        "pd.concat([res1,res2,res3,res4,res5,res6,res7])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWIwUCktrnE4"
      },
      "source": [
        "#### Exercise\n",
        "Try to beat the performace of the best network with your own neutal network.  \n",
        "*Hint: You might want to combine things from the neural networks above*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoTLI7sErojX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "08_cifar10_tricks",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}